name: Token Management System Tests

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  schedule:
    # Run tests daily at 2 AM UTC
    - cron: '0 2 * * *'

env:
  PYTHON_VERSION: '3.11'
  NODE_VERSION: '18'

jobs:
  test:
    name: Run Tests
    runs-on: ubuntu-latest
    
    strategy:
      matrix:
        test-type: [unit, integration, performance, security, e2e]
        python-version: ['3.9', '3.10', '3.11']
        exclude:
          # Exclude some combinations to reduce CI time
          - test-type: e2e
            python-version: '3.9'
          - test-type: performance
            python-version: '3.9'
          - test-type: performance
            python-version: '3.10'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements*.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov pytest-asyncio pytest-mock
        pip install coverage
        pip install black isort flake8 mypy
        pip install httpx aiohttp
        pip install pydantic
        pip install sqlalchemy
        pip install psycopg2-binary
        pip install python-dotenv
        pip install requests
        pip install pandas numpy
        pip install plotly dash
        pip install fastapi uvicorn
        pip install openai anthropic
    
    - name: Lint code
      run: |
        black --check src/ tests/
        isort --check-only src/ tests/
        flake8 src/ tests/
        mypy src/
    
    - name: Run ${{ matrix.test-type }} tests
      run: |
        python scripts/run_tests.py --type ${{ matrix.test-type }} --verbose --coverage
      continue-on-error: true
    
    - name: Upload test results
      if: always()
      uses: actions/upload-artifact@v3
      with:
        name: test-results-${{ matrix.test-type }}-${{ matrix.python-version }}
        path: |
          test_results.json
          coverage/
          *.log
        retention-days: 30
    
    - name: Generate coverage report
      if: matrix.test-type == 'unit'
      run: |
        python scripts/coverage_report.py --modules token_management --output-format html
    
    - name: Upload coverage report
      if: matrix.test-type == 'unit'
      uses: actions/upload-artifact@v3
      with:
        name: coverage-report-${{ matrix.python-version }}
        path: coverage/
        retention-days: 30

  coverage:
    name: Coverage Analysis
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov coverage
    
    - name: Run comprehensive coverage analysis
      run: |
        python scripts/coverage_report.py --modules token_management --output-format all
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage/coverage_report.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: true

  security:
    name: Security Tests
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install security dependencies
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety semgrep
    
    - name: Run bandit security scan
      run: |
        bandit -r src/ -f json -o bandit-report.json
      continue-on-error: true
    
    - name: Run safety check
      run: |
        safety check --json --output safety-report.json
      continue-on-error: true
    
    - name: Run semgrep security scan
      run: |
        semgrep --config=auto --json --output=semgrep-report.json src/
      continue-on-error: true
    
    - name: Upload security scan results
      uses: actions/upload-artifact@v3
      with:
        name: security-scan-results
        path: |
          bandit-report.json
          safety-report.json
          semgrep-report.json
        retention-days: 30

  performance:
    name: Performance Regression Test
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-benchmark
    
    - name: Run performance benchmarks
      run: |
        python -m pytest tests/test_token_performance.py --benchmark-only --benchmark-json=benchmark-results.json
    
    - name: Compare with baseline
      run: |
        python -c "
        import json
        import sys
        
        with open('benchmark-results.json', 'r') as f:
            current_results = json.load(f)
        
        # Load baseline (if exists)
        baseline_file = 'baseline-benchmark.json'
        if os.path.exists(baseline_file):
            with open(baseline_file, 'r') as f:
                baseline_results = json.load(f)
            
            # Compare results
            for test_name, current_data in current_results.items():
                if test_name in baseline_results:
                    baseline_data = baseline_results[test_name]
                    current_mean = current_data.get('mean', 0)
                    baseline_mean = baseline_data.get('mean', 0)
                    
                    if current_mean > baseline_mean * 1.5:  # 50% slower
                        print(f'PERFORMANCE REGRESSION: {test_name}')
                        print(f'  Current: {current_mean:.4f}s')
                        print(f'  Baseline: {baseline_mean:.4f}s')
                        print(f'  Increase: {((current_mean - baseline_mean) / baseline_mean * 100):.1f}%')
                        sys.exit(1)
        
        print('Performance regression test passed')
        "
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: |
          benchmark-results.json
          baseline-benchmark.json
        retention-days: 30

  integration:
    name: Integration Tests
    runs-on: ubuntu-latest
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_PASSWORD: testpassword
          POSTGRES_DB: token_management_test
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-asyncio pytest-mock
        pip install psycopg2-binary
        pip install httpx aiohttp
        pip install python-dotenv
    
    - name: Wait for PostgreSQL
      run: |
        until pg_isready -h localhost -p 5432; do
          echo "Waiting for PostgreSQL..."
          sleep 2
        done
    
    - name: Run integration tests
      run: |
        export DATABASE_URL=postgresql://postgres:testpassword@localhost:5432/token_management_test
        python scripts/run_tests.py --type integration --verbose
    
    - name: Upload integration test results
      uses: actions/upload-artifact@v3
      with:
        name: integration-test-results
        path: |
          test_results.json
          *.log
        retention-days: 30

  e2e:
    name: End-to-End Tests
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-asyncio pytest-mock
        pip install httpx aiohttp
        pip install fastapi uvicorn
        pip install requests
    
    - name: Start test services
      run: |
        # Start mock services for E2E testing
        python -m pytest tests/test_token_management_complete.py --collect-only > /dev/null
        echo "Test services started"
    
    - name: Run end-to-end tests
      run: |
        python scripts/run_tests.py --type e2e --verbose
    
    - name: Upload E2E test results
      uses: actions/upload-artifact@v3
      with:
        name: e2e-test-results
        path: |
          test_results.json
          *.log
        retention-days: 30

  report:
    name: Test Report
    runs-on: ubuntu-latest
    needs: [test, coverage, security, performance, integration, e2e]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Download all artifacts
      uses: actions/download-artifact@v3
    
    - name: Generate comprehensive test report
      run: |
        python -c "
        import json
        import os
        from datetime import datetime
        
        # Generate test summary
        report = {
            'timestamp': datetime.now().isoformat(),
            'total_jobs': 6,
            'completed_jobs': 0,
            'failed_jobs': 0,
            'test_results': {}
        }
        
        # Process test results
        for artifact in os.listdir('.'):
            if os.path.isdir(artifact) and 'test-results' in artifact:
                json_file = os.path.join(artifact, 'test_results.json')
                if os.path.exists(json_file):
                    with open(json_file, 'r') as f:
                        results = json.load(f)
                        report['test_results'][artifact] = results
                        report['completed_jobs'] += 1
                        if not all(r['success'] for r in results.values()):
                            report['failed_jobs'] += 1
        
        # Save comprehensive report
        with open('comprehensive_test_report.json', 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f'Test Summary:')
        print(f'  Total Jobs: {report[\"total_jobs\"]}')
        print(f'  Completed: {report[\"completed_jobs\"]}')
        print(f'  Failed: {report[\"failed_jobs\"]}')
        print(f'  Success Rate: {(report[\"completed_jobs\"] - report[\"failed_jobs\"]) / report[\"completed_jobs\"] * 100:.1f}%')
        "
    
    - name: Upload comprehensive report
      uses: actions/upload-artifact@v3
      with:
        name: comprehensive-test-report
        path: |
          comprehensive_test_report.json
        retention-days: 90
    
    - name: Notify on failure
      if: needs.test.result == 'failure' || needs.coverage.result == 'failure' || needs.security.result == 'failure' || needs.performance.result == 'failure' || needs.integration.result == 'failure' || needs.e2e.result == 'failure'
      uses: 8398a7/action-slack@v3
      with:
        status: failure
        fields: repo,message,commit,author,action,eventName,ref,workflow
        text: 'Token Management System tests failed!'
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}